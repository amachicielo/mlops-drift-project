
## 🧠 MLOps Drift Detection Pipeline with Airflow, FastAPI & MLflow

![Docker Compose](https://img.shields.io/badge/docker-compose-blue)
![Python](https://img.shields.io/badge/python-3.10-blue)
![License](https://img.shields.io/badge/license-MIT-green)

A professional MLOps pipeline for detecting data drift, retraining models, and managing experiments using **Airflow**, **FastAPI**, **Docker**, and **MLflow**.

---


## 📚 Table of Contents

- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Setup Instructions](#setup-instructions)
- [Usage](#usage)
- [Monitoring & Retraining](#monitoring--retraining)
- [Project Structure](#project-structure)
- [Technologies Used](#technologies-used)
- [License](#license)

---


## 🧠 Project Overview

This project is a full MLOps pipeline for monitoring data drift and automatically retraining a machine learning model. It includes:

- A local environment orchestrated with Docker.
- Model training and evaluation using MLflow.
- Drift detection using Evidently.
- Scheduled pipeline orchestration with Apache Airflow.
- An API for making predictions via FastAPI.

---


## 🏗️ Architecture

The architecture follows a modular MLOps approach:

- `data/`: stores datasets.
- `src/training/`: scripts for training and saving the model.
- `src/drift_detection/`: scripts for detecting data drift.
- `src/api/`: FastAPI application for predictions.
- `airflow/dags/`: Airflow DAGs for scheduling tasks.
- `mlruns/`: MLflow experiment tracking.
- `reports/`: Drift reports generated by Evidently.
- `docker-compose.yml`: defines services for training and API.
- `docker-compose.airflow.yml`: services for Airflow orchestration.

---


## 🛠️ Technologies Used

This project integrates several modern tools for a production-ready MLOps setup:

- **Python 3.10**: Main programming language.
- **Pandas / Scikit-learn**: For data preprocessing and model training.
- **Evidently AI**: For data drift detection and monitoring.
- **FastAPI**: For serving the ML model via a REST API.
- **MLflow**: For model tracking and experiment logging.
- **Docker / Docker Compose**: For containerization and service orchestration.
- **Apache Airflow**: For automating pipeline tasks and re-training.

---


## 🚀 How to Run (Local Setup)

1. **Clone the repository**  
   ~bash
   git clone https://github.com/your-user/mlops-drift-project.git
   cd mlops-drift-project

2. **Start all services**
Use the provided PowerShell script to run everything (API, MLflow, Airflow, etc.):
    ~powershell
    .\start.ps1

3. **Access the interfaces**
API: http://localhost:8000/docs
MLflow UI: http://localhost:5000
Airflow UI: http://localhost:8080

- Username: admin
- Password: admin

4. **Trigger drift monitoring manually**
~bash
docker-compose exec api python auto_monitor.py

---


## 🔁 Usage Workflow

This project automates a real MLOps pipeline focused on data drift detection and retraining. Here's how it works:

1. **Initial training**
   - A logistic regression model is trained using the `bank.csv` dataset.
   - The model and its metrics are logged to MLflow.

2. **Drift detection**
   - A script (`auto_monitor.py`) compares new data against a reference sample.
   - If significant drift is detected in one or more features, a retraining is triggered.

3. **Model retraining and versioning**
   - A new model is trained and logged to MLflow with a new run ID.
   - Accuracy and run metadata are automatically tracked.

4. **Airflow orchestration**
   - All steps can be triggered and monitored using DAGs in Airflow.
   - This makes it easy to schedule daily/weekly checks, automate retraining, or chain downstream actions.

5. **API for predictions**
   - A FastAPI service is available to serve the model and make real-time predictions from JSON inputs.


---

## 📁 Project Structure

Here's an overview of the main directories and files:

- `mlops-drift-project/`
  - `airflow/`
    - `dags/`
      - `mlops_dag.py` — Airflow main DAG
    - `api/`
      - `main.py` — FastAPI API for prediction
    - `requirements.txt` — Airflow environment dependencies
    - `Dockerfile` — Dockerfile for the Airflow container
  - `data/`
    - `bank.csv` — Main dataset used
    - `bank.zip` — Original dataset
  - `mlruns/` — MLflow experiments
    - `models/`
      - `random_forest.pkl` — Trained model
  - `reports/` — HTML reports generated by Evidently
    - `drift_auto_report*.html`
    - `drift_report*.html`
  - `src/`
    - `drift_detection/`
      - `monitor_drift.py` — Drift detection
    - `inference/` — (Empty, reserved for API use)
    - `training/`
      - `train_model.py` — Initial training
  - `tests/`
    - `test_api.py`, `test_training.py`
  - `docker-compose.yml` — API + MLflow
  - `docker-compose.airflow.yml` — Full stack (Airflow + Redis + Postgres)
  - `Dockerfile` — Dockerfile for the API
  - `README.md` — Project documentation
  - `requirements.txt` — General requirements (for the API)
  - Scripts:
    - `start.ps1` — Start full environment
    - `stop.ps1` — Stop containers
    - `restart.ps1` — Restart services
    - `rebuild.ps1` — Rebuild Docker images
    - `reset_db.ps1` — Reset the Airflow database


---


## 🔧 Installation

This project requires Docker and Docker Compose to be installed on your machine.

### 1. Clone the repository

~bash
git clone https://github.com/your-username/mlops-drift-project.git
cd mlops-drift-project

### 2. Start the full stack (API, MLflow, Airflow, Redis, Postgres)
For Windows users (PowerShell):

~powershell
.\start.ps1

This script will:
- Build all Docker images
- Start API, MLflow, Airflow, Redis and Postgres containers
- Initialize Airflow database if needed

If this is your first time running the project, you may need to run reset_db.ps1 first to reset Airflow database.

### 3. Stop all services
~powershell
.\stop.ps1

### 4. Rebuild services (after code changes)
~powershell
.\rebuild.ps1

### 5. Reset Airflow DB (optional, useful for clean experiments)
~powershell
.\reset_db.ps1

---


## 🚀 Usage

Once the stack is up, you can access the following services:

- 📡 **FastAPI**: http://localhost:8000/docs
- 📈 **MLflow Tracking UI**: http://localhost:5000
- ⏳ **Airflow UI**: http://localhost:8080 (default login: `admin` / `admin`)

### 🧪 Run Drift Detection Manually

To run the Evidently drift report manually from inside the container:

~powershell
docker-compose exec api python src/drift_detection/monitor_drift.py

The output report will be saved in the reports/ folder as HTML.

🤖 Trigger Auto Monitoring + Retraining
This script detects drift and retrains the model if necessary:

~powershell
docker-compose exec api python src/drift_detection/auto_monitor.py

If drift is detected in critical columns, it will:
- Generate a drift report
- Retrain the model using src/training/train_model.py
- Log the new model in MLflow

📅 Scheduled Execution via Airflow
The project includes an Airflow DAG (drift_monitoring_dag.py) that:

- Periodically checks for drift
- Retrains the model if drift is detected

To activate it:
1. Go to Airflow UI: http://localhost:8080
2. Enable the DAG named drift_monitoring_pipeline
3. Trigger manually or wait for the scheduled interval


### 📡 FastAPI - Example Usage

Once the API is running at http://localhost:8000, you can test the prediction endpoint.

Swagger UI (interactive docs): [http://localhost:8000/docs](http://localhost:8000/docs)

#### Sample request

~bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
        "age": 35,
        "job": "technician",
        "marital": "married",
        "education": "secondary",
        "default": "no",
        "housing": "yes",
        "loan": "no",
        "contact": "cellular",
        "month": "may",
        "day_of_week": "mon",
        "duration": 150,
        "campaign": 1,
        "pdays": 999,
        "previous": 0,
        "poutcome": "nonexistent",
        "emp.var.rate": 1.1,
        "cons.price.idx": 93.994,
        "cons.conf.idx": -36.4,
        "euribor3m": 4.857,
        "nr.employed": 5191
      }'

You will receive a JSON response with the predicted class and probability.


### ⚙️ Configuration

---

## ⚙️ Configuration

### Environment Variables

These are the key variables used in the services (already defined in the Docker files):

| Variable                              | Description                      | Default             |
|---------------------------------------|----------------------------------|---------------------|
| `MLFLOW_TRACKING_URI`                 | MLflow server URI                | http://mlflow:5000  |
| `API_PORT`                            | Port for the FastAPI service     | 8000                |
| `AIRFLOW__CORE__EXECUTOR`             | Airflow executor type            | CeleryExecutor      |
| `AIRFLOW__DATABASE__SQL_ALCHEMY_CONN` | PostgreSQL URL                   | postgresql+psycopg2://airflow:airflow@postgres/airflow |

You can change them directly in the `docker-compose*.yml` files or in `.env` (if supported).

### Ports Used

Make sure the following ports are available:

- **8000** - FastAPI
- **5000** - MLflow UI
- **8080** - Airflow Webserver
- **8793** - Airflow Worker
- **5432** - PostgreSQL
- **6379** - Redis


---


> By John G. Amachi Cielo

> MLOps educational and professional project
